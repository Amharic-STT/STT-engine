{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c34a6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile #for audio processing\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf88600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f219ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_augmentation import Data_Augmentation\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b39d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad939635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, translations):\n",
    "        self.translations = translations\n",
    "        self.unk = -1\n",
    "        \n",
    "    def build_dict(self):\n",
    "        text = ''\n",
    "        for t in self.translations:\n",
    "            text += t\n",
    "        \n",
    "        char_counts = Counter(text)\n",
    "        sorted_vocab = sorted(char_counts, key=char_counts.get, reverse=True)\n",
    "        int_to_char = {ii: word for ii, word in enumerate(sorted_vocab, 1)}\n",
    "\n",
    "        char_to_int = {word: ii for ii, word in int_to_char.items()}\n",
    "        \n",
    "        return int_to_char, char_to_int\n",
    "    \n",
    "    def encode(self, sent, char_to_int):\n",
    "        \n",
    "        encoded = []\n",
    "        char_list = list(sent)\n",
    "        for c in char_list:\n",
    "            try:\n",
    "                encoded.append(char_to_int[c])\n",
    "            \n",
    "            except KeyError:\n",
    "                encoded.append(self.unk)\n",
    "        return encoded\n",
    "    \n",
    "    def decode_text(self, encoded_chars, int_to_char):\n",
    "        \n",
    "        decoded = ''\n",
    "        for e in encoded_chars:\n",
    "            try:\n",
    "                decoded += e\n",
    "            \n",
    "            except KeyError:\n",
    "                decoded += ''\n",
    "        \n",
    "        return decoded\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5040c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,  translations, audios, batch_size=32, shuffle=True):\n",
    "        self.audios = audios\n",
    "        self.labels = translations\n",
    "        self.batch_size = batch_size\n",
    "        self.len = int(np.floor(len(self.labels) / self.batch_size))\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.tokenizer = Tokenizer(translations)\n",
    "        self.int_to_char, self.char_to_int = tokenizer.build_dict()\n",
    "        \n",
    "        self.cur_index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def encode_text(self, translations):\n",
    "        encoded_trans =  []\n",
    "        \n",
    "        for t in translations:\n",
    "            encoded = self.tokenizer.encode(t, self.char_to_int)\n",
    "            encoded_trans.append(encoded)\n",
    "        \n",
    "        return encoded_trans\n",
    "    \n",
    "    def get_max_len(self, items):\n",
    "        maximum = 0\n",
    "        for i in items:\n",
    "            if len(i) > maximum:\n",
    "                maximum = len(i)\n",
    "                \n",
    "        return maximum\n",
    "\n",
    "            \n",
    "    def __data_generation(self, batch_translations, batch_audios):\n",
    "     \n",
    "        self.cur_index = 0\n",
    "        encoded_trans = self.encode_text(batch_translations)\n",
    "        \n",
    "        maximum_trans_len = self.get_max_len(encoded_trans)\n",
    "        maximum_audio_len = self.get_max_len(batch_audios)\n",
    "        \n",
    "        \n",
    "        encoded_trans_np = np.zeros((len(encoded_trans), maximum_trans_len), dtype=\"int64\")\n",
    "        padded_audios_np = np.zeros((len(batch_audios), maximum_audio_len), dtype=\"float32\")\n",
    "        \n",
    "        label_length = np.zeros(padded_audios_np.shape[0], dtype=\"int64\")\n",
    "        input_length = np.zeros(encoded_trans_np.shape[0], dtype=\"int64\")\n",
    "        \n",
    "        \n",
    "        ind = 0\n",
    "        for trans, audio in zip(encoded_trans, batch_audios):\n",
    "            encoded_trans_np[ind,0:len(trans)] = trans\n",
    "            label_length[ind] = len(trans)\n",
    "            \n",
    "            padded_audio = np.pad(audio, (0, maximum_audio_len - len(audio)), mode = 'constant', constant_values=0)\n",
    "            \n",
    "            padded_audios_np[ind, ] = padded_audio\n",
    "            input_length[ind] = len(audio)\n",
    "            \n",
    "            ind += 1\n",
    "        \n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        inputs = {'the_input':   tf.convert_to_tensor(padded_audios_np), \n",
    "                  'the_labels':   tf.convert_to_tensor(encoded_trans_np), \n",
    "                  'input_length':   tf.convert_to_tensor(input_length), \n",
    "                  'label_length':   tf.convert_to_tensor(label_length) \n",
    "                 }\n",
    "        \n",
    "        return (inputs, outputs)\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "                \n",
    "        self.indexes = np.arange(self.len*self.batch_size)\n",
    "\n",
    "        if self.shuffle == True:\n",
    "\n",
    "            self.indexes = self.indexes.reshape(int(self.len), int(self.batch_size))\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            for i in range(self.len):\n",
    "                np.random.shuffle(self.indexes[i])\n",
    "\n",
    "            self.indexes = self.indexes.reshape(int(self.len*self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[int(index*self.batch_size):int((index+1)*self.batch_size)]\n",
    "        \n",
    "        self.cur_index += self.batch_size\n",
    "        \n",
    "        if  self.cur_index >= len(self.labels):\n",
    "            self.cur_index = 0\n",
    "\n",
    "        batch_labels = [self.labels[int(k)] for k in indexes]\n",
    "        batch_audios = [self.audios[int(k)] for k in indexes]\n",
    "        \n",
    "        batch_labels = self.labels[self.cur_index:  self.cur_index + self.batch_size]\n",
    "        batch_audios = self.audios[ self.cur_index:  self.cur_index + self.batch_size]\n",
    "    \n",
    "        \n",
    "        return  self.__data_generation(batch_labels, batch_audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae42d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogMelSpectrogram(tf.keras.layers.Layer):\n",
    "    \"\"\"Compute log-magnitude mel-scaled spectrograms.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate, fft_size, hop_size, n_mels,\n",
    "                 f_min=0.0, f_max=None, **kwargs):\n",
    "        super(LogMelSpectrogram, self).__init__(**kwargs)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.fft_size = fft_size\n",
    "        self.hop_size = hop_size\n",
    "        self.n_mels = n_mels\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max if f_max else sample_rate / 2\n",
    "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=self.n_mels,\n",
    "            num_spectrogram_bins=fft_size // 2 + 1,\n",
    "            sample_rate=self.sample_rate,\n",
    "            lower_edge_hertz=self.f_min,\n",
    "            upper_edge_hertz=self.f_max)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.mel_filterbank)\n",
    "        super(LogMelSpectrogram, self).build(input_shape)\n",
    "\n",
    "    def call(self, waveforms):\n",
    "        \"\"\"Forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : tf.Tensor, shape = (None, n_samples)\n",
    "            A Batch of mono waveforms.\n",
    "        Returns\n",
    "        -------\n",
    "        log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)\n",
    "            The corresponding batch of log-mel-spectrograms\n",
    "        \"\"\"\n",
    "        def _tf_log10(x):\n",
    "            numerator = tf.math.log(x)\n",
    "            denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "            return numerator / denominator\n",
    "\n",
    "        def power_to_db(magnitude, amin=1e-16, top_db=80.0):\n",
    "            \"\"\"\n",
    "            https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html\n",
    "            \"\"\"\n",
    "            ref_value = tf.reduce_max(magnitude)\n",
    "            log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))\n",
    "            log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))\n",
    "            log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
    "\n",
    "            return log_spec\n",
    "\n",
    "        spectrograms = tf.signal.stft(waveforms,\n",
    "                                      frame_length=self.fft_size,\n",
    "                                      frame_step=self.hop_size,\n",
    "                                      pad_end=False)\n",
    "\n",
    "        magnitude_spectrograms = tf.abs(spectrograms)\n",
    "\n",
    "        mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),\n",
    "                                     self.mel_filterbank)\n",
    "\n",
    "        log_mel_spectrograms = power_to_db(mel_spectrograms)\n",
    "\n",
    "        # add channel dimension\n",
    "        log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\n",
    "\n",
    "        return log_mel_spectrograms\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'fft_size': self.fft_size,\n",
    "            'hop_size': self.hop_size,\n",
    "            'n_mels': self.n_mels,\n",
    "            'sample_rate': self.sample_rate,\n",
    "            'f_min': self.f_min,\n",
    "            'f_max': self.f_max,\n",
    "        }\n",
    "        config.update(super(LogMelSpectrogram, self).get_config())\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c8fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessin_model(sample_rate, fft_size, frame_step, n_mels, mfcc=False):\n",
    "\n",
    "    input_data = Input(name='input', shape=(None,), dtype=\"float32\")\n",
    "    featLayer = LogMelSpectrogram(\n",
    "        fft_size=fft_size,\n",
    "        hop_size=frame_step,\n",
    "        n_mels=n_mels,\n",
    "        \n",
    "        sample_rate=sample_rate,\n",
    "        f_min=0.0,\n",
    "        \n",
    "        f_max=int(sample_rate / 2)\n",
    "    )(input_data)\n",
    "    \n",
    "    x = BatchNormalization()(featLayer)\n",
    "    model = Model(inputs=input_data, outputs=x, name=\"preprocessin_model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d82a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidirectionalRNN(input_dim, batch_size, sample_rate=22000,\n",
    "                     rnn_layers=2, units=400, drop_out=0.5, act='tanh', output_dim=224):\n",
    "\n",
    "    input_data = Input(name='the_input', shape=(\n",
    "        None, input_dim), batch_size=batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    x = Bidirectional(LSTM(units,  activation=act,\n",
    "                      return_sequences=True, implementation=2))(input_data)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "\n",
    "    for i in range(rnn_layers - 2):\n",
    "        x = Bidirectional(\n",
    "            LSTM(units, activation=act, return_sequences=True))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(drop_out)(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(units,  activation=act,\n",
    "                      return_sequences=True, implementation=2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(x)\n",
    "\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=y_pred, name=\"BidirectionalRNN\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf0de92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn_model(input_dim, output_dim=224):\n",
    "\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    simp_rnn = GRU(output_dim, return_sequences=True,\n",
    "                   implementation=2, name='rnn')(input_data)\n",
    "    y_pred = Activation('softmax', name='softmax')(simp_rnn)\n",
    "    model = Model(inputs=input_data, outputs=y_pred, name=\"simple_rnn_model\")\n",
    "    model.output_length = lambda x: x\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0488fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e1accf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_lengths_lambda_func(args):\n",
    "    hop_size = frame_step\n",
    "    input_length = args\n",
    "    return tf.cast(tf.math.ceil(input_length/hop_size)-1, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14d575a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ctc_loss(model_builder):\n",
    "    the_labels      = Input(name='the_labels',      shape=(None,), dtype='float32')\n",
    "    input_lengths   = Input(name='input_length',    shape=(1,), dtype='float32')\n",
    "    label_lengths   = Input(name='label_length',    shape=(1,), dtype='float32')\n",
    "\n",
    "    input_lengths2 = Lambda(input_lengths_lambda_func)(input_lengths)\n",
    "    if model_builder.output_length:\n",
    "         output_lengths  = Lambda(model_builder.output_length)(input_lengths2) - 1\n",
    "    else:\n",
    "         output_lengths  = input_lengths2\n",
    "    \n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([model_builder.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model( inputs=[model_builder.input, the_labels, input_lengths, label_lengths],  outputs=loss_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7abeb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_builder, \n",
    "          data_len,\n",
    "          data_gen,\n",
    "          batch_size = 25,\n",
    "          epochs=20, \n",
    "          verbose=1,\n",
    "          optimizer=SGD(learning_rate=0.002, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "          ):    \n",
    "              \n",
    "    model = add_ctc_loss(model_builder)\n",
    "\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    hist = model.fit_generator(generator=data_gen,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose, \n",
    "                               use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f654a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_obj = helper.read_obj(\"../data/translation_dict.pkl\")\n",
    "audio_obj = helper.read_obj(\"../data/audio_dict.pkl\")\n",
    "# meta_data = data_loader.create_meta_data(translation_obj, audio_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef6a4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for label in audio_obj:\n",
    "    audios.append(audio_obj[label][0])\n",
    "    \n",
    "translations = []\n",
    "for label in audio_obj:\n",
    "    translations.append(translation_obj[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "107cd033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample snt: የተለያዩ የ ትግራይ አውራጃ ተወላጆች ገንዘባቸው ን አዋጥ ተው የ ልማት ተቋማትን እንዲ መሰርቱ ትልማ አይ ፈቅድ ም\n",
      "encoded snt: [7, 8, 11, 6, 131, 1, 7, 1, 3, 28, 27, 24, 1, 10, 4, 27, 115, 1, 8, 37, 29, 149, 18, 1, 21, 2, 65, 23, 26, 4, 1, 2, 1, 10, 41, 43, 1, 8, 4, 1, 7, 1, 12, 22, 3, 1, 8, 88, 22, 3, 2, 1, 13, 2, 49, 1, 15, 31, 14, 69, 1, 3, 12, 22, 1, 10, 24, 1, 61, 45, 32, 1, 16]\n",
      "decoed snt: የተለያዩ የ ትግራይ አውራጃ ተወላጆች ገንዘባቸው ን አዋጥ ተው የ ልማት ተቋማትን እንዲ መሰርቱ ትልማ አይ ፈቅድ ም\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(translations)\n",
    "int_to_char, char_to_int = tokenizer.build_dict()\n",
    "sample = translations[0]\n",
    "encoded = tokenizer.encode(sample, char_to_int)\n",
    "decoded = tokenizer.decode_text(sample, encoded)\n",
    "\n",
    "print(f\"sample snt: {sample}\")\n",
    "print(f\"encoded snt: {encoded}\")\n",
    "print(f\"decoed snt: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdea9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_rate = 22000\n",
    "fft_size = 1024\n",
    "frame_step = 512\n",
    "n_mels = 128\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "data_len = len(translations)\n",
    "output_dim = len(char_to_int) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb25dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"preprocessin_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "log_mel_spectrogram (LogMelS (None, None, 128, 1)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 128, 1)      4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 2\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dg = DataGenerator(translations, audios, batch_size)\n",
    "preprocess_model = preprocessin_model(sample_rate, fft_size, frame_step, n_mels)\n",
    "preprocess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e43ae703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, None, 128)]       0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 223)         236157    \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 223)         0         \n",
      "=================================================================\n",
      "Total params: 236,157\n",
      "Trainable params: 236,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "speech_model = simple_rnn_model(n_mels, output_dim)\n",
    "speech_model.summary()\n",
    "# speech_model = BidirectionalRNN(n_mels, output_dim=output_dim)\n",
    "# speech_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3675d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(output_dim, custom_model, preprocess_model, mfcc=False, calc=None):\n",
    "\n",
    "    input_audios = Input(name='the_input', shape=(None,))\n",
    "    pre = preprocess_model(input_audios)\n",
    "    pre = tf.squeeze(pre, [3])\n",
    "\n",
    "    y_pred = custom_model(pre)\n",
    "    model = Model(inputs=input_audios, outputs=y_pred, name=\"model_builder\")\n",
    "    model.output_length = calc\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8e25a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_builder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "preprocessin_model (Function (None, None, 128, 1)      4         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLa (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_model (Functional (None, None, 223)         236157    \n",
      "=================================================================\n",
      "Total params: 236,161\n",
      "Trainable params: 236,159\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(output_dim, speech_model, preprocess_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01feee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "preprocessin_model (Functional) (None, None, 128, 1) 4           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLambd (None, None, 128)    0           preprocessin_model[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "simple_rnn_model (Functional)   (None, None, 223)    236157      tf.compat.v1.squeeze[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           input_length[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           simple_rnn_model[0][0]           \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 lambda[0][0]                     \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 236,161\n",
      "Trainable params: 236,159\n",
      "Non-trainable params: 2\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 127s 2s/step - loss: 309.7716\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 127s 3s/step - loss: 284.1927\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 128s 3s/step - loss: 282.6456\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 124s 2s/step - loss: 281.3853\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 115s 2s/step - loss: 280.0898\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 123s 2s/step - loss: 278.9373\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 131s 3s/step - loss: 277.7985\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 149s 3s/step - loss: 276.7455\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 160s 3s/step - loss: 275.8950\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 157s 3s/step - loss: 275.1489\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 153s 3s/step - loss: 274.4764\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 154s 3s/step - loss: 273.8633\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 157s 3s/step - loss: 273.3033\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 145s 3s/step - loss: 272.7712\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 140s 3s/step - loss: 272.2827\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 150s 3s/step - loss: 271.8903\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 148s 3s/step - loss: 271.4583\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 133s 3s/step - loss: 271.0945\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 144s 3s/step - loss: 270.7689\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 148s 3s/step - loss: 270.4774\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_experiment('Speech Model-RNN-baseline')\n",
    "# mlflow.tensorflow.autolog()\n",
    "train(model, 100, dg, epochs=20,  batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d2a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
