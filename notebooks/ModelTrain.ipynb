{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34a6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile #for audio processing\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf88600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f219ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_generator import DataGenerator\n",
    "from tokenizer import Tokenizer\n",
    "from logspectrorgam import LogMelSpectrogram\n",
    "from bidirectionalRNN import BidirectionalRNN\n",
    "from simpleRNN import simple_rnn_model\n",
    "from ctc_loss import add_ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b39d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c8fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessin_model(sample_rate, fft_size, frame_step, n_mels, mfcc=False):\n",
    "\n",
    "    input_data = Input(name='input', shape=(None,), dtype=\"float32\")\n",
    "    featLayer = LogMelSpectrogram(\n",
    "        fft_size=fft_size,\n",
    "        hop_size=frame_step,\n",
    "        n_mels=n_mels,\n",
    "        \n",
    "        sample_rate=sample_rate,\n",
    "        f_min=0.0,\n",
    "        \n",
    "        f_max=int(sample_rate / 2)\n",
    "    )(input_data)\n",
    "    \n",
    "    x = BatchNormalization()(featLayer)\n",
    "    model = Model(inputs=input_data, outputs=x, name=\"preprocessin_model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d82a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidirectionalRNN(input_dim, batch_size, sample_rate=22000,\n",
    "                     rnn_layers=2, units=400, drop_out=0.5, act='tanh', output_dim=224):\n",
    "\n",
    "    input_data = Input(name='the_input', shape=(\n",
    "        None, input_dim), batch_size=batch_size)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    x = Bidirectional(LSTM(units,  activation=act,\n",
    "                      return_sequences=True, implementation=2))(input_data)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "\n",
    "    for i in range(rnn_layers - 2):\n",
    "        x = Bidirectional(\n",
    "            LSTM(units, activation=act, return_sequences=True))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(drop_out)(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(units,  activation=act,\n",
    "                      return_sequences=True, implementation=2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(x)\n",
    "\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=y_pred, name=\"BidirectionalRNN\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf0de92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn_model(input_dim, output_dim=224):\n",
    "\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    simp_rnn = GRU(output_dim, return_sequences=True,\n",
    "                   implementation=2, name='rnn')(input_data)\n",
    "    y_pred = Activation('softmax', name='softmax')(simp_rnn)\n",
    "    model = Model(inputs=input_data, outputs=y_pred, name=\"simple_rnn_model\")\n",
    "    model.output_length = lambda x: x\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abeb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_builder, \n",
    "          data_len,\n",
    "          data_gen,\n",
    "          batch_size = 25,\n",
    "          epochs=20, \n",
    "          verbose=1,\n",
    "          optimizer=SGD(learning_rate=0.002, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "          ):    \n",
    "              \n",
    "    model = add_ctc_loss(model_builder)\n",
    "\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    hist = model.fit_generator(generator=data_gen,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose, \n",
    "                               use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f654a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_obj = helper.read_obj(\"../data/translation_dict.pkl\")\n",
    "audio_obj = helper.read_obj(\"../data/audio_dict.pkl\")\n",
    "# meta_data = data_loader.create_meta_data(translation_obj, audio_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6a4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = []\n",
    "for label in audio_obj:\n",
    "    audios.append(audio_obj[label][0])\n",
    "    \n",
    "translations = []\n",
    "for label in audio_obj:\n",
    "    translations.append(translation_obj[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107cd033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample snt: የተለያዩ የ ትግራይ አውራጃ ተወላጆች ገንዘባቸው ን አዋጥ ተው የ ልማት ተቋማትን እንዲ መሰርቱ ትልማ አይ ፈቅድ ም\n",
      "encoded snt: [7, 8, 11, 6, 131, 1, 7, 1, 3, 28, 27, 24, 1, 10, 4, 27, 115, 1, 8, 37, 29, 149, 18, 1, 21, 2, 65, 23, 26, 4, 1, 2, 1, 10, 41, 43, 1, 8, 4, 1, 7, 1, 12, 22, 3, 1, 8, 88, 22, 3, 2, 1, 13, 2, 49, 1, 15, 31, 14, 69, 1, 3, 12, 22, 1, 10, 24, 1, 61, 45, 32, 1, 16]\n",
      "decoed snt: የተለያዩ የ ትግራይ አውራጃ ተወላጆች ገንዘባቸው ን አዋጥ ተው የ ልማት ተቋማትን እንዲ መሰርቱ ትልማ አይ ፈቅድ ም\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(translations)\n",
    "int_to_char, char_to_int = tokenizer.build_dict()\n",
    "sample = translations[0]\n",
    "encoded = tokenizer.encode(sample, char_to_int)\n",
    "decoded = tokenizer.decode_text(sample, encoded)\n",
    "\n",
    "print(f\"sample snt: {sample}\")\n",
    "print(f\"encoded snt: {encoded}\")\n",
    "print(f\"decoed snt: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdea9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_rate = 22000\n",
    "fft_size = 1024\n",
    "frame_step = 512\n",
    "n_mels = 128\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "data_len = len(translations)\n",
    "output_dim = len(char_to_int) + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb25dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"preprocessin_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "log_mel_spectrogram (LogMelS (None, None, 128, 1)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 128, 1)      4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 2\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dg = DataGenerator(translations, audios, batch_size)\n",
    "preprocess_model = preprocessin_model(sample_rate, fft_size, frame_step, n_mels)\n",
    "preprocess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43ae703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, None, 128)]       0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 223)         236157    \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 223)         0         \n",
      "=================================================================\n",
      "Total params: 236,157\n",
      "Trainable params: 236,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "speech_model = simple_rnn_model(n_mels, output_dim)\n",
    "speech_model.summary()\n",
    "# speech_model = BidirectionalRNN(n_mels, output_dim=output_dim)\n",
    "# speech_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3675d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(output_dim, custom_model, preprocess_model, mfcc=False, calc=None):\n",
    "\n",
    "    input_audios = Input(name='the_input', shape=(None,))\n",
    "    pre = preprocess_model(input_audios)\n",
    "    pre = tf.squeeze(pre, [3])\n",
    "\n",
    "    y_pred = custom_model(pre)\n",
    "    model = Model(inputs=input_audios, outputs=y_pred, name=\"model_builder\")\n",
    "    model.output_length = calc\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e25a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_builder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "preprocessin_model (Function (None, None, 128, 1)      4         \n",
      "_________________________________________________________________\n",
      "tf.compat.v1.squeeze (TFOpLa (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_model (Functional (None, None, 223)         236157    \n",
      "=================================================================\n",
      "Total params: 236,161\n",
      "Trainable params: 236,159\n",
      "Non-trainable params: 2\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(output_dim, speech_model, preprocess_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01feee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment('Speech Model-RNN-baseline')\n",
    "# mlflow.tensorflow.autolog()\n",
    "train(model, 100, dg, epochs=20,  batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d2a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
